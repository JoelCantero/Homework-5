---
title: "Homework 5"
author: "Marc Mendez & Joel Cantero"
date: "5th May, 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```


First of all, we are going to install and load all the libraries we need for this exercise.

```{r installing-packages, message=FALSE, results=FALSE, warning=FALSE}
packages <- c("rpart","ROCR", "readxl", "randomForest", "mice")
for (package in packages) {
    if(!require(package,  character.only=TRUE)){
        install.packages(package, repos="http://cran.rstudio.com")
        library(package,  character.only=TRUE)
    } 
}
```

## 1. Read the Audit.xlsx file and convert it to the csv extension.

Once we have loaded and installed all the packages, we are going to load the excel file. Also, we are going to convert Employment. Education, Marital, Occupation, Gender, Accounts and Adjusted attributes to factors. Then, we will convert it to CSV file thanks to write.csv2 function.

```{r load-excel}
audit <- read_excel("audit.xlsx", na = "NA")
factors <- c("Employment", "Education", "Marital", "Occupation", "Gender",
             "Accounts", "Adjusted")
for (fac in factors) {
  audit[fac] <- lapply(audit[fac], factor)
}
write.csv2(audit, "audit.csv")
```


## 2. The goal is to use a decision tree to predict the binary "Adjusted" variable, whether the individuals had made a correct financial statement or not. Decide which predictors you would use and eventually preprocess these variables.

First of all, we have to remove these attributes that will not help us for splitting the tree. We can observe that "ID", "Deductions" and "Adjusted" are related to statement made and we do not need them. After that, we will imput missing values with MICE function. We have found 244 missing values and we have decided to use MICE because it is a small percentatge of all our instances.

```{r deciding-predictors, echo=FALSE, message=FALSE, results=FALSE, warning=FALSE, out.width="300px", out.height="150px", fig.align="center" }
audit <- subset(audit, select = -c(ID, Deductions, Adjustment))
sum(is.na(audit)) # We found 244 missing values.
md.pattern(audit)
res <- mice(audit,  m = 1, method="cart")
imputedAudit <- complete(res, 1)
sum(is.na(imputedAudit)) # Now is 0.
md.pattern(imputedAudit)
```
So we can see that with MICE we solved all the missing values, and now we can proceed with our study.

## 3. Select the 1/3 of the last observations as test data.

We have to select the last 33% of data instances as test, and the first 66% instances as training data. 

```{r selecting-data}
test <- imputedAudit[seq(0.66*nrow(imputedAudit), nrow(imputedAudit)), ]
training <- imputedAudit[-seq(0.66*nrow(imputedAudit), nrow(imputedAudit)), ]
```

## 4. Obtain the decision tree to predict whether the variable "Adjusted" on the training data. Decide the cutoff value for taking the decision.

```{r obtain-decision-tree, echo=FALSE }

rt = rpart(formula = Adjusted ~ .,
           data = training,
           method="class",
           control = rpart.control(xval = 10,
                                   cp = 0.001))
printcp(rt)
cutoffIdx = 1
minVal = 20         
cptab = as.data.frame(rt$cptable)
for (i in seq(1, nrow(cptab))) {
    row = cptab[i, ]
    tmpSum = row$xerror + row$xstd
    if(tmpSum <= minVal) {
        minVal = tmpSum
        cutoffIdx = i
    }
}
```
If we calculate the cutoff and with the cutoff we look at the table we obtain the following:
```{r obtain-decision-tree-v2, echo=FALSE }
cat("Cutoff idx:", cutoffIdx , " With min value:", minVal)
cat("CP:", cptab[cutoffIdx,]$CP, " nsplit:", cptab[cutoffIdx, ]$nsplit, "rel error:",cptab[cutoffIdx, ]$`rel error`, "xerror:", cptab[cutoffIdx, ]$xerror, "xstd:", cptab[cutoffIdx, ]$xstd )
```
In our case the cutoff will be on 5 splits.
## 5. Plot the importance of variables in the prediction.


```{r importance-variables, echo=FALSE, out.width="300px", out.height="150px", fig.align="center"}
imp = rt$variable.importance
barplot(imp, col = "grey", las=2)
```

As we can see in this plot, the three most important variables are: marital, occupation and income. 

## 6. Compute the accuracy, precision, recall and AUC on the test individuals.
Before calculating the accuracy, precision, recall and AUC, we need to calculate the confusion matrix. After it we can start calculating the variables said earlier.
```{r accuracy-precision, out.width="300px", out.height="150px", fig.align="center"}
prediction <- predict(rt, test, type="class")
(results <- table(test$Adjusted, prediction))

# Accuracy
error <- (results[1,2] + results[2,1])/nrow(test) 
accuracy <- 1 -  error 
# Precision 
precision_p <- results[1,1]/(results[1,1] + results[2,1])
precision_n <- results[2,2]/(results[1,2] + results[2,2])
precision <- (precision_n + precision_p)/2
# Recall
recall <- results[1,1]/(results[1,1] + results[1,2])
```

```{r accuracy-precision-v2, echo = FALSE, out.width="300px", out.height="150px", fig.align="center"}
# ROC (auc)
prd<- as.data.frame(predict(rt, test)) 
roc_prediction <- prediction(prd$`1`, test$Adjusted)
roc <- performance(roc_prediction,measure="tpr",x.measure="fpr")
plot(roc, main="ROC curve")
abline(0,1,col="blue")
```



We obtain a very high accuracy. If we look at the amount of results predicted we see that our precision is also high, and last but not least, the recall value which is also very high.
```{r accuracy-precision-v3, echo=FALSE, message=FALSE, warning=FALSE, out.width="300px", out.height="150px", fig.align="center"}
auc <- performance(roc_prediction,"auc")
aucv <- auc@y.values

cat("Accuracy: ", accuracy)
cat("Precision: ", precision)
cat("Recall: ", recall)
cat("AUC: ", aucv[[1]])
```



## 7. Perform a Random Forest on the same data.

As we did previously, we have to build the model and then test the results with the test data (the same split as we have used before, if we want to compare it).

The goal of this exercise is to compare the previous results with a random forest. For this reason, we will calculate again the accuracy, precision, recall and AUC.

```{r random-forest, echo=FALSE, message=FALSE, out.width="300px", out.height="150px", fig.align="center"}
randomForest <- randomForest(formula = Adjusted ~ .,
             data=training,
             importance=TRUE)
print(randomForest)
predictRandomForest <- predict(randomForest, test)

#print(predictRandomForest)
plot(predictRandomForest)

results <- table(test$Adjusted, predictRandomForest)

# Accuracy
error <- (results[1,2] + results[2,1])/nrow(test) 
accuracy <- 1 -  error # 82.96 accuracy

# Precision 
precision_p <- results[1,1]/(results[1,1] + results[2,1])
precision_n <- results[2,2]/(results[1,2] + results[2,2])
precision <- (precision_n + precision_p)/2

# Recall
recall <- results[1,1]/(results[1,1] + results[1,2])


# ROC

rf_roc <- predict(randomForest, test, type="prob")[,2]
rf_roc_prediction <- prediction(rf_roc, test$Adjusted)
rf_roc_perf <- performance(rf_roc_prediction, "tpr", "fpr")

plot(rf_roc_perf, main="ROC curve")
abline(0,1,col="blue")



```

```{r accuracy-precision-v4, echo=FALSE, message=FALSE, warning=FALSE}
auc <- performance(rf_roc_prediction,"auc")
aucv <- auc@y.values
cat("Accuracy: ", accuracy)
cat("Precision: ", precision)
cat("Recall: ", recall)
cat("AUC: ", aucv[[1]])
```

##Conclusions

To conclude, we can say that all the metrics have been improved using a random forest just using 500 trees (we can see that if we print randomForest variable). If we just use one decision tree (the previous exercise) against a random forest, we can observe that the results are not good as random forest ones. 